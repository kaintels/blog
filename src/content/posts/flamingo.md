---
title: Flamingo ë¦¬ë·°
published: 2024-10-25
description: "Flamingo: a Visual Language Model for Few-Shot Learning"
tags: [Research]
category: Review
draft: false
---

## ì›ë³¸ ë…¼ë¬¸

[Flamingo: a Visual Language Model for Few-Shot Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html)

Advances in neural information processing systems 2022

## ìš”ì•½
>
> FlamingoğŸ¦©ë¼ëŠ” ë¹„ì£¼ì–¼ ì–¸ì–´ ëª¨ë¸ì„ ì†Œê°œ
ì´ ëª¨ë¸ì€ ì£¼ì–´ì§„ ì†Œìˆ˜ì˜ ì˜ˆì‹œë¡œ ìƒˆë¡œìš´ ì‘ì—…ì„ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ê³  ìˆìœ¼ë©°, í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€(ë¹„ë””ì˜¤)ë¥¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµí•¨. ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ íƒì›”í•œ ì„±ê³¼ë¥¼ ë³´ì˜€ìœ¼ë©°, ìƒˆë¡œìš´ ë°ì´í„° ì—†ì´ë„ ê°„ë‹¨í•œ ì˜ˆì‹œë“¤ë¡œ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ì£¼ìš” ê¸°ì—¬ì 

## 1. ë„ì…ë¶€

í˜„ì¬ ì»´í“¨í„° ë¹„ì „ì—ì„œ ìƒˆë¡œìš´ ì‘ì—…ì„ ë¹ ë¥´ê²Œ ë°°ìš°ëŠ” ê²ƒì— ëŒ€í•œ ì—°êµ¬ê°€ ì˜ ìˆ˜í–‰ë˜ê³  ìˆì§€ë§Œ, ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ë“¤ì€ ì—¬ì „íˆ ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ ì‚¬ìš©í•´ ì‚¬ì „ í•™ìŠµí•œ í›„, ê´€ì‹¬ ìˆëŠ” ì‘ì—…ì— ë§ê²Œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ì‹ì— ì˜ì¡´í•˜ê³  ìˆìŒ. ê·¸ëŸ¬ë‚˜ ì„±ê³µì ì¸ ë¯¸ì„¸ ì¡°ì •ì„ ìœ„í•´ì„œëŠ” ìˆ˜ì²œ ê°œì˜ ì£¼ì„ì´ ë‹¬ë¦° ë°ì´í„° í¬ì¸íŠ¸ê°€ í•„ìš”í•˜ë©°, ì‘ì—…ë³„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ì´ í•„ìš”í•  ë¿ë§Œ ì•„ë‹ˆë¼ ìì›ì´ ë§ì´ ì†Œëª¨ë¨.

ìµœê·¼ì—ëŠ” ëŒ€ì¡°ì  ëª©ì (contrastive objective) ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµëœ ë‹¤ì¤‘ ëª¨ë‹¬ ë¹„ì „-ì–¸ì–´ ëª¨ë¸ì´ ë¯¸ì„¸ ì¡°ì • ì—†ì´ë„ ìƒˆë¡œìš´ ì‘ì—…ì— ì œë¡œìƒ· ì ì‘ì„ ê°€ëŠ¥í•˜ê²Œ í•¨. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ëª¨ë¸ì€ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ ìœ ì‚¬ì„± ì ìˆ˜ë§Œì„ ì œê³µí•  ìˆ˜ ìˆì–´ ë¶„ë¥˜ì™€ ê°™ì€ ì œí•œëœ ì‚¬ìš© ì‚¬ë¡€ì—ë§Œ ì ìš©ë  ìˆ˜ ìˆëŠ” í•œê³„ê°€ ìˆìŒ.

ì´ë“¤ì€ ì–¸ì–´ ìƒì„± ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ì—¬ ìº¡ì…”ë‹ì´ë‚˜ ë¹„ì£¼ì–¼ ì§ˆë¬¸ ì‘ë‹µê³¼ ê°™ì€ ì‘ì—…ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŒ. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì‹œê°ì ì¸ ë‚´ìš©ì— ëŒ€í•œ ì–¸ì–´ ìƒì„±ì— ëŒ€í•œ íƒêµ¬ë„ ì´ë£¨ì–´ì¡Œì§€ë§Œ, ì•„ì§ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì§€ ëª»í•˜ê³  ìˆìŒ.

ë³¸ ë…¼ë¬¸ì€ ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê³ , ë‹¤ì–‘í•œ ë¹„ì „ ë° ì–¸ì–´ ì‘ì—…ì—ì„œ ì†Œìˆ˜ì˜ ì˜ˆì‹œë§Œìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë¹„ì£¼ì–¼ ì–¸ì–´ ëª¨ë¸ì¸ Flamingoë¥¼ ì†Œê°œí•¨. ì´ ëª¨ë¸ì€ ëª‡ ê°€ì§€ ì…ë ¥/ì¶œë ¥ ì˜ˆì‹œë§Œìœ¼ë¡œ ë‹¤ì–‘í•œ ì‘ì—…ì— ì ìš©ë  ìˆ˜ ìˆìœ¼ë©°, ë¯¸ì„¸ ì¡°ì • ì—†ì´ë„ ì—¬ëŸ¬ ì‘ì—…ì—ì„œ ìƒˆë¡œìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ.

## Flamingo ê²°ê³¼ ì˜ˆì‹œ

(ë¬´ìŠ¨ ë™ë¬¼ì— ëŒ€í•œ ëŒ€ë‹µ)
![](https://velog.velcdn.com/images/kaintels/post/c22cd4e3-8548-43bc-99f3-702df3302fb9/image.png)

(ë™ë¬¼ ìˆ˜ì— ëŒ€í•œ ëŒ€ë‹µ)
![](https://velog.velcdn.com/images/kaintels/post/18b0304f-0f28-45a0-8496-9a101f5e1898/image.png)

(ì˜ìƒ ë‚´ìš© ì„¤ëª…)
![](https://velog.velcdn.com/images/kaintels/post/c1dd7751-2e15-4064-a917-db3cabcced63/image.png)

## 2. ìƒì„¸ì„¤ëª…

![](https://velog.velcdn.com/images/kaintels/post/2d646839-f089-4760-8090-5ee4a74cd1ce/image.png)

FlamingoëŠ” ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ **Vision encoder**, **Language model(LM) block**ë¡œ ë‚˜ë‰˜ë©°, **Perceiver Resampler**(ì—°ë³´ë¼ìƒ‰), **Gated XATTN-Dense**ê°€ ìˆëŠ” ê²ƒì´ íŠ¹ì§•

Vision encoder : CLIP text-image contrastive learningìœ¼ë¡œ í•™ìŠµëœ ë¹„ì „ ì¸ì½”ë”ë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•¨

Language model : Large text corpusë¡œ í•™ìŠµëœ Chinchillaë¥¼ ì‚¬ì „í•™ìŠµ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•¨

Perceiver Resampler : ë¹„ì „ ì¸ì½”ë”ì˜ ì¶œë ¥ì„ ê³ ì •ëœ í¬ê¸°ë¡œ mappingí•˜ëŠ” ì—­í• (ê³ ì • í¬ê¸°ë§Œí¼ ì‘ì•„ì ¸ì„œ ê³„ì‚° íš¨ìœ¨ì )

Gated XATTN-Dense : Query, Key, Valueë¥¼ ì…ë ¥ ë°›ì•„ í•´ë‹¹ ì •ë³´ê°€ ê°€ë¯¸ëœ ë²¡í„°ë¥¼ ì¶œë ¥í•˜ëŠ” ë ˆì´ì–´

ë‹¤ìŒìœ¼ë¡œ Perceiver Resamplerì™€ Gated XATTN-Denseì— ëŒ€í•´ì„œ ì„¤ëª…

#### Perceiver Resampler

![](https://velog.velcdn.com/images/kaintels/post/7f2c3696-ce7f-4a41-8302-6c8234a0f4f6/image.png)

ìš°ì„  Perceiver ResamplerëŠ” ë³¸ ë…¼ë¬¸(2022)ì—ì„œ ìƒˆë¡­ê²Œ ì œì•ˆí•œ ê±´ ì•„ë‹ˆê³  2021ë…„ [Perceiver: General Perception with Iterative Attention](https://proceedings.mlr.press/v139/jaegle21a.html)ì—ì„œ ì œì•ˆë¨. ë³¸ ë…¼ë¬¸ì€ Cross Attentionì— ì…ë ¥í•˜ê¸° ìœ„í•œ ë¦¬ìƒ˜í”Œë§ ë°©ë²•ì„ ì‚¬ìš©í•œ ê²ƒì´ë¼ ë³´ë©´ ë¨.

ìš°ì„  ì„¤ëª… ì „ì—, Flamingo ëª¨ë¸ì—ì„œëŠ” ìš°ì„  Vision Featureì™€ Text Featureë¥¼ ê°™ì´ ì—°ì‚°í•´ì£¼ì–´ í•œë‹¤ëŠ” ê±¸ ìƒê°í•´ë³´ì.

ì´ë•Œ Vision FeatureëŠ” Text Featureë³´ë‹¤ ì¼ë°˜ì ìœ¼ë¡œ í›¨ì”¬ í° ì°¨ì›ì„ ê°–ê¸° ë•Œë¬¸ì— ì´ ë‘˜ì„ ë™ì‹œì— ì—°ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” Vision Featureì˜ ì°¨ì›ì„ Text Featureì— ë§ê²Œ ì¶•ì†Œí•´ì¤„ í•„ìš”ê°€ ìˆìŒ. (ì•ˆ ê·¸ëŸ¼ ë²¡í„°ì˜ ì°¨ì›ë„ ì•ˆ ë§ê³  ì—°ì‚° ìì²´ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤)

ìœ„ ê·¸ë¦¼ì—ì„œëŠ” ì•„ë˜ìª½ì—ì„œ ê³ ì°¨ì› ë²¡í„°ì— í•´ë‹¹í•˜ëŠ” Vision Featureë¥¼ ì´ˆë¡ìƒ‰ ê³„ì—´ë¡œ í‘œí˜„í•˜ê³  ìˆëŠ”ë°, ì´ì œ ì´ Vision Featureë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì¶•ì†Œí•´ì•¼ í•¨. (ì´ë•Œì˜ íƒ€ê²Ÿ ë²¡í„°(Latent)ë¥¼ íšŒìƒ‰ìœ¼ë¡œ í‘œí˜„)

Queryë¡œ ë“¤ì–´ê°€ëŠ” ì €ì°¨ì› ë²¡í„°ëŠ” Learned Latent Vector(ê·¸ë¦¼ì˜ ìœ„ì¹˜ì •ë³´ë¥¼ ì‹ ê²½ë§ì— ì…ë ¥í•´ì„œ ë‚˜ì˜¨ ë²¡í„°)ë¥¼ ì‚¬ìš©í•¨. ì´ë ‡ê²Œ Learned Latent VectorëŠ” Queryë¡œ, Vision Featureë¥¼ Key, Valueë¡œ í•˜ì—¬ Cross Attentionì„ ìˆ˜í–‰

#### Gated XATTN-Dense

![](https://velog.velcdn.com/images/kaintels/post/7719548b-a46b-4db7-b3ce-e40c49f619e3/image.png)

Gated XATTN-DenseëŠ” ì•ì„œ Vision ì •ë³´ì™€ Language ì •ë³´ë¥¼ ìœµí•©(Cross Attention)í•˜ì—¬ Languageê°€ ê°€ë¯¸ëœ ë¹„ì „ ë²¡í„°ë¥¼ ì–»ëŠ”ë‹¤ê³  ìƒê°í•˜ë©´ í¸í•¨. ì—¬ê¸°ì„œ ì…ë ¥ìœ¼ë¡œ ìš”êµ¬ë˜ëŠ” Query, Key, Valueë¥¼ ì •í•´ì•¼í•œë‹¤. ë³¸ ëª¨ë¸ì€ ë¹„ì£¼ì–¼-ì–¸ì–´ ëª¨ë¸ì´ë¯€ë¡œ ìµœì¢… ì¶œë ¥ì€ í…ìŠ¤íŠ¸ì„. ë”°ë¼ì„œ Query ë¡œëŠ” Text Feature, ì •ë³´ë¥¼ ì¶”ê°€í•  Key, Valueë¡œëŠ” Vision Featureë¥¼ ì‚¬ìš©

ë³¸ ë…¼ë¬¸ì— ì‚¬ìš©í•˜ëŠ” ì—°ì‚° ìˆœì„œëŠ” ìœ„ ê·¸ë¦¼ì„ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆìŒ. ì„¤ëª…í•˜ìë©´ ë¨¼ì € visual feature(x)ì— ëŒ€í•´ Cross Attention(q=y(language), kv=x) ì—°ì‚° ë’¤ì— feed forward(FFW) ë ˆì´ì–´ë¥¼ í†µí•´ weightì™€ biasë¥¼ (wx+b)ê³„ì‚°í•˜ê³ , ì´í›„ ê·¸ ê°’(y)ì— ë‹¤ì‹œ self attenction(q=y, kv=y)ë¥¼ ì—°ì‚°, FFW ë ˆì´ì–´ë¥¼ ê±°ì³ ìµœì¢… yë¥¼ ì¶œë ¥í•¨

## 3. ì‹¤í—˜ ê²°ê³¼

ë³¸ ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ë°ì´í„° ì…‹ì€ ì•„ë˜ì™€ ê°™ìŒ

- M3W (MultiModal MassiveWeb): ì´ ë°ì´í„°ì…‹ì€ 4,300ë§Œ ê°œì˜ ì›¹í˜ì´ì§€ì—ì„œ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° ì›¹í˜ì´ì§€ì—ì„œ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ìœ„ì¹˜ ê´€ê³„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œê° ë°ì´í„°ë¥¼ ì¶”ì¶œ
- ALIGN: 18ì–µ ê°œì˜ ì´ë¯¸ì§€ì™€ ëŒ€ì²´ í…ìŠ¤íŠ¸(alt-text) ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹
- LTIP (Long Text & Image Pairs): 3ì–µ 1,200ë§Œ ê°œì˜ ì´ë¯¸ì§€ì™€ ê¸´ ì„¤ëª… í…ìŠ¤íŠ¸ ìŒìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹
- VTP (Video & Text Pairs): í‰ê·  22ì´ˆ ê¸¸ì´ì˜ 2,700ë§Œ ê°œì˜ ì§§ì€ ë¹„ë””ì˜¤ì™€ í•´ë‹¹ ë¹„ë””ì˜¤ì— ëŒ€í•œ ë¬¸ì¥ ì„¤ëª…ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹

ì´í›„ ì•„ë˜ ë‘ ì¡°ê±´ìœ¼ë¡œ ì‹¤í—˜ ì§„í–‰

#### 1. Few-shot learning on vision-language tasks

ë³¸ ë…¼ë¬¸ì€ Few Shot Learningì— ëŒ€í•œ ê²°ê³¼ë¥¼ ìœ„í•´ ì´ 16ê°œì˜ ë²¤ì¹˜ë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ì¸¡ì •. ê° ë¬¸ì œë³„(ë²¤ì¹˜ë§ˆí¬)ë§ˆë‹¤ ëª¨ë¸ì„ Fine Tuning í•˜ì§€ ì•Šê³  ë‹¨ì§€ ì‚¬ì „ í•™ìŠµì„ ì™„ë£Œí•œ Flamingoì—ê²Œ ëª‡ ê°€ì§€ ì˜ˆì‹œë¥¼ ì œê³µí•˜ê³  í•´ê²°í•˜ë„ë¡ í•¨

![](https://velog.velcdn.com/images/kaintels/post/f95677ec-fac8-41a0-9460-5ce867f8a7eb/image.png)

ì‹¤í—˜ ê²°ê³¼ ê¸°ì¡´ SOTA ëª¨ë¸ë“¤ ëŒ€ë¹„ ì„±ëŠ¥ì´ ê±°ì˜ ëª¨ë‘ ì¢‹ì•˜ë‹¤ê³  í•¨.

#### 2. Fine-tuning Flamingo as a pretrained vision-language model

![](https://velog.velcdn.com/images/kaintels/post/abe773b2-0d0f-4980-8d5a-95eb69d671a0/image.png)

ì´ë²ˆì—ëŠ” ë¬¸ì œë“¤ì— ëŒ€í•´ ì¶”ê°€ í•™ìŠµì„ í•˜ì—¬(Fine-tune) ì„±ëŠ¥ì„ ì‚°ì¶œí•œ ê²°ê³¼, ëŒ€ë‹¤ìˆ˜ì˜ ê²°ê³¼ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„

## 4. ê²°ë¡ 

ì´ ë…¼ë¬¸ì˜ ê²°ë¡ ì—ì„œ ì €ìë“¤ì€ Flamingo ëª¨ë¸ì„ ì œì•ˆí•˜ë©°, ì´ë¯¸ì§€ ë° ë¹„ë””ì˜¤ ì‘ì—…ì— ìµœì†Œí•œì˜ ì‘ì—…ë³„ í›ˆë ¨ ë°ì´í„°ë§Œìœ¼ë¡œ ì ìš© ê°€ëŠ¥í•œ ë²”ìš© ëª¨ë¸ì´ë¼ê³  ì„¤ëª…í•¨.

FlamingoëŠ” ì „í†µì ì¸ ë¹„ì „ ë²¤ì¹˜ë§ˆí¬ë¥¼ ë„˜ì–´ ëŒ€í™”ì™€ ê°™ì€ ìƒí˜¸ì‘ìš© ê¸°ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, ë‹¤ì–‘í•œ ì‹œê°ì  ì‘ì—…ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ì˜€ê³ , ë˜í•œ, ì‚¬ì „ í•™ìŠµëœ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ê³¼ ê°•ë ¥í•œ ë¹„ì „ ëª¨ë¸ì„ ì—°ê²°í•˜ëŠ” ê²ƒì´ ë²”ìš© ì‹œê° ì´í•´ë¡œ ë‚˜ì•„ê°€ëŠ” ì¤‘ìš”í•œ ë‹¨ê³„ì„ì„ ê°•ì¡°í•¨.

## ì‚¬ìš©ë°©ë²•

[github](https://github.com/mlfoundations/open_flamingo)ë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ì¹˜ ë° í™œìš©í•  ìˆ˜ ìˆìŒ.

```python
# grab model checkpoint from huggingface hub
from huggingface_hub import hf_hub_download
from PIL import Image
import requests
import torch

checkpoint_path = hf_hub_download("openflamingo/OpenFlamingo-3B-vitl-mpt1b", "checkpoint.pt")
model.load_state_dict(torch.load(checkpoint_path), strict=False)

"""
Step 1: Load images
"""
demo_image_one = Image.open(
    requests.get(
        "http://images.cocodataset.org/val2017/000000039769.jpg", stream=True
    ).raw
)

demo_image_two = Image.open(
    requests.get(
        "http://images.cocodataset.org/test-stuff2017/000000028137.jpg",
        stream=True
    ).raw
)

query_image = Image.open(
    requests.get(
        "http://images.cocodataset.org/test-stuff2017/000000028352.jpg", 
        stream=True
    ).raw
)


"""
Step 2: Preprocessing images
Details: For OpenFlamingo, we expect the image to be a torch tensor of shape 
 batch_size x num_media x num_frames x channels x height x width. 
 In this case batch_size = 1, num_media = 3, num_frames = 1,
 channels = 3, height = 224, width = 224.
"""
vision_x = [image_processor(demo_image_one).unsqueeze(0), image_processor(demo_image_two).unsqueeze(0), image_processor(query_image).unsqueeze(0)]
vision_x = torch.cat(vision_x, dim=0)
vision_x = vision_x.unsqueeze(1).unsqueeze(0)

"""
Step 3: Preprocessing text
Details: In the text we expect an <image> special token to indicate where an image is.
 We also expect an <|endofchunk|> special token to indicate the end of the text 
 portion associated with an image.
"""
tokenizer.padding_side = "left" # For generation padding tokens should be on the left
lang_x = tokenizer(
    ["<image>An image of two cats.<|endofchunk|><image>An image of a bathroom sink.<|endofchunk|><image>An image of"],
    return_tensors="pt",
)


"""
Step 4: Generate text
"""
generated_text = model.generate(
    vision_x=vision_x,
    lang_x=lang_x["input_ids"],
    attention_mask=lang_x["attention_mask"],
    max_new_tokens=20,
    num_beams=3,
)

print("Generated text: ", tokenizer.decode(generated_text[0]))
```
